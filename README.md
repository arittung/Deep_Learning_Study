# 딥러닝 스터디

[arittung.log - 딥러닝 스터디 시리즈](https://velog.io/@arittung/series/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EC%8A%A4%ED%84%B0%EB%94%94) 참고.

위 링크에 작성된 내용에 기반한 코드.<br><br><br>

## 목차
- [Day01](/혼자%20공부하는%20머신러닝%2B딥러닝/Day01.ipynb)
  - [인공지능과 머신러닝, 딥러닝이란?](#인공지능과-머신러닝,-딥러닝이란?)
  - [퍼셉트론](#퍼셉트론)
- [Day02](/혼자%20공부하는%20머신러닝%2B딥러닝/Day02.ipynb)
  - [지도 학습과 비지도 학습, 강화 학습](#지도-학습과-비지도-학습-강화-학습)
  - [k-최근접 이웃 알고리즘(K-Nearest Neighbor)](#k-최근접-이웃-알고리즘(K-Nearest-Neighbor))
- [Day03](/혼자%20공부하는%20머신러닝%2B딥러닝/Day03.ipynb)
  - [데이터 전처리](#데이터-전처리)
  - [Day03.pynb에 대한 설명](#day03pynb에-대한-설명)
- [Day04](/혼자%20공부하는%20머신러닝%2B딥러닝/Day04.ipynb)
  - [k-최근접 이웃 회귀 (KNN-Regression)](#k-최근접-이웃-회귀-knn-regression)
- [Day05](/혼자%20공부하는%20머신러닝%2B딥러닝/Day05.ipynb)
  - [선형 회귀 (Linear-Regression)](#선형-회귀-linear-regression)
- [Day06](/혼자%20공부하는%20머신러닝%2B딥러닝/Day06.ipynb)
  - [다중 회귀 (Multiple-Regression)](#다중-회귀-multiple-regression)
  - [특성 공학 (Feature Engineering)](#특성-공학-feature-engineering)
  - [판다스 (Pandas)](#판다스-pandas)
  - [규제 (Regularization)](#규제-regularization)
- [Day07](/혼자%20공부하는%20머신러닝%2B딥러닝/Day07.ipynb)
  - [다중 분류 (Multi-Class Classification)](#다중-분류-multi-class-classification)
  - [로지스틱 회귀 (Logistic Regression)](#로지스틱-회귀-logistic-regression)
  - [시그모이드 함수와 소프트맥스 함수](#시그모이드-함수와-소프트맥스-함수)
- [Day08](/혼자%20공부하는%20머신러닝%2B딥러닝/Day08.ipynb)
  - [경사하강법 (Gradient Descent)](#경사하강법-gradient-descent-1)
  - [확률적 경사 하강법 (SGD, Stochastic Grandient Descent)](#확률적-경사-하강법-sgd-stochastic-grandient-descent-1)
  - [미니 배치 경사 하강법 (Mini-batch Gradient Descent)](#미니-배치-경사-하강법-mini-batch-gradient-descent-1)
  - [손실함수 (Loss Function, 비용함수(Cost Function))](#손실함수-loss-function-비용함수cost-function-1)
- [Day09](/혼자%20공부하는%20머신러닝%2B딥러닝/Day09.ipynb)
  - [결정 트리 (Decision Tree, 의사 결정 트리)](결정-트리-decision-tree-의사-결정-트리)
- [Day10](/혼자%20공부하는%20머신러닝%2B딥러닝/Day10.ipynb)
  - [검증 세트 (Validation Set)](#검증-세트-validation-set)
  - [교차 검증 (Cross Validation)](#교차-검증-cross-validation)
  - [하이퍼파라미터 (HyperParameter)](#하이퍼파라미터-hyperparameter)
  - [랜덤 서치 (Random Search)](#랜덤-서치-random-search)
- [Day11](/혼자%20공부하는%20머신러닝%2B딥러닝/Day11.ipynb)
  - [정형 데이터와 비정형 데이터](#정형-데이터와-비정형-데이터)
  - [앙상블 학습 (Ensemble Learning)](#앙상블-학습-ensemble-learning)
  - [랜덤 포레스트 (Random Forest)](#랜덤-포레스트-random-forest)
  - [엑스트라 트리 (Extra Tree)](#엑스트라-트리-extra-tree)
  - [그레이디언트 부스팅 (Gradient Boosting)](#그레이디언트-부스팅-gradient-boosting)
  - [히스토그램 기반 그레이디언트 부스팅 (Histogram based Gradient Boosting)](#히스토그램-기반-그레이디언트-부스팅-histogram-based-gradient-boosting)
- [Day12](/혼자%20공부하는%20머신러닝%2B딥러닝/Day12.ipynb)
  - [군집 (Clustering, 클러스터링)](#군집-clustering-클러스터링)


<br>
<br>

---

<br>

## Day01
#### 인공지능과 머신러닝, 딥러닝이란?
  - 인공지능
  - 머신러닝
  - 딥러닝
 
#### 퍼셉트론
  - 퍼셉트론
  - 단층 퍼셉트론
  - 단층 퍼셉트론 구현 코드
  - 다층 퍼셉트론(MLP, Multi-Layer Perceptron)

<br>

## Day02
#### 지도 학습과 비지도 학습, 강화 학습
  - 지도 학습(Supervised Learning)
    - 분류(Classification)
    - 회귀(Regression)
  - 비지도 학습(unsupervised learning)
  - 강화 학습(Reinforcement Learning)
 
#### k-최근접 이웃 알고리즘(K-Nearest Neighbor)
  - KNN 구현 코드

<br>

## Day03
#### 데이터 전처리
  - 전처리 과정에서 해야 하는 일
  - 주요 데이터 전처리 기법
#### Day03.pynb에 대한 설명
  - 전처리 전
  - 전처리 후

<br>


## Day04
#### k-최근접 이웃 회귀 (KNN-Regression)
  - 결정 계수 (Coefficient of determination, R^2)
  - 과대적합(Overfitting) & 과소적합(Underfitting)
    - 과대 적합(Overfitting)
    - 과소 적합(Underfitting)
  - KNN-Regression 구현 코드

 <br>


## Day05
#### 선형 회귀 (Linear-Regression)
  - 손실(Loss)
    - 평균 제곱 오차(MSE, Mean Squared Error)
    - 평균 절대 오차(MAE, Mean Absolute Error)
  - 경사 하강법 (Gradient Descent)
  - 수렴(Convergence)
  - 과소 적합(Underfitting)
  - 학습률 (Learning Rate)
  - 다항 회귀 (Polynomial Regression)
  - 구현 코드

 <br>


## Day06
#### 다중 회귀 (Multiple Regression)
#### 특성 공학 (Feature Engineering)
#### 판다스 (Pandas)
#### 규제 (Regularization)
  - 규제의 필요성
  - 릿지 회귀 (Ridge Regression)와 라쏘 회귀 (Lasso Regression), 엘라스틱 넷 회귀 (ElasticNet Regression)
    - 릿지 회귀
    - 라쏘 회귀
    - 엘라스틱 넷 회귀

 <br>

## Day07
#### 다중 분류 (Multi-Class Classification)
#### 로지스틱 회귀 (Logistic Regression)
#### 시그모이드 함수와 소프트맥스 함수
  - 시그모이드 함수 (Sigmoid Function, 로지스틱 함수(Logistic Function))
  - 소프트맥스 함수 (SoftMax Function)
  
 <br>

## Day08
#### 경사하강법 (Gradient Descent)
  - 경사하강법 Step Size
  - 경사하강법 진행 순서
  - 경사하강법의 문제점과 해결 방법
    - 문제점
    - 해결방법: 모멘텀 (Momentum)
  - 배치 사이즈 (Batch Size)
#### 확률적 경사 하강법 (SGD, Stochastic Grandient Descent)
  - 특징
  - 에포크 (Epoch)
#### 미니 배치 경사 하강법 (Mini-batch Gradient Descent)
#### 손실함수 (Loss Function, 비용함수(Cost Function))
  - 평균 제곱 오차와 크로스 엔트로피
    - 평균 제곱 오차(Mean Square Error, MSE)
    - 교차 엔트로피 오차 (Cross Entropy Error, CEE)
  - 로지스틱 손실 함수 (Logistic loss Function, 이진 크로스엔트로피 손실 함수(Binary cross-entropy loss function))
 

 <br>

## Day09
#### 결정 트리 (Decision Tree, 의사 결정 트리)
   - 결정 트리 구조
   - 결정 트리 알고리즘의 프로세스
   - 불순도 (Impurity)와 지니 불순도 (Gini Impurity)
   - 정보 이득 (Information Gain)
   - 가지치기 (Pruning)

 <br>

## Day10
#### 검증 세트 (Validation Set)
  - 검증 세트 사용 방법
#### 교차 검증 (Cross Validation)
#### 하이퍼파라미터 (HyperParameter)
#### 랜덤 서치 (Random Search)


 <br>

## Day11
#### 정형 데이터와 비정형 데이터
  - 정형 데이터 (Structed Data)
  - 비정형 데이터 (Unstructured Data)
#### 앙상블 학습 (Ensemble Learning)
#### 랜덤 포레스트 (Random Forest)
  - 부트스트랩 샘플 (Bootstrap Sample)
#### 엑스트라 트리 (Extra Tree)
#### 그레이디언트 부스팅 (Gradient Boosting)
#### 히스토그램 기반 그레이디언트 부스팅 (Histogram based Gradient Boosting)



 <br>

## Day12
#### 군집 (Clustering, 클러스터링)
  - 클러스터 (Cluster, 부분 그룹)
  - 센트로이드 (Centroid, 클러스터 중심(Cluster Center))














<br><br>
---
<br>

## 참고 도서
<img src="https://images.velog.io/images/arittung/post/de70472d-133c-44cb-b17f-a66d9fada811/image.png" width="150px">

**[한빛미디어] 혼자 공부하는 머신러닝+딥러닝, 박해선 저**


